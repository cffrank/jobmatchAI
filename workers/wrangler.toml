# JobMatch AI - Cloudflare Workers Configuration
# https://developers.cloudflare.com/workers/wrangler/configuration/

name = "jobmatch-ai"
main = "api/index.ts"
compatibility_date = "2024-12-01"
compatibility_flags = ["nodejs_compat"]

# Build configuration
[build]
command = "npm install"

# Workers AI binding for embeddings
# Uses @cf/baai/bge-base-en-v1.5 model (768-dimensional vectors)
# Free tier: Included in Workers plan
# Docs: https://developers.cloudflare.com/workers-ai/
[ai]
binding = "AI"

# Environment-specific configurations
# Development
[env.development]
name = "jobmatch-ai-dev"
vars = { ENVIRONMENT = "development" }

# Staging
[env.staging]
name = "jobmatch-ai-staging"
vars = { ENVIRONMENT = "staging" }

# Production
[env.production]
name = "jobmatch-ai-prod"
vars = { ENVIRONMENT = "production" }

# Scheduled Jobs (Cron Triggers) - DISABLED due to account limit
# https://developers.cloudflare.com/workers/configuration/cron-triggers/
# Note: Account already has 5+ cron triggers from other workers
# Cleanup tasks can be triggered manually via API if needed
# [env.production.triggers]
# crons = [
#   # Hourly cleanup (rate limits, OAuth states, failed logins, unlock accounts)
#   "0 * * * *",
#   # Daily at 3 AM UTC - archive old jobs
#   "0 3 * * *"
# ]

# KV Namespaces
# Note: Create namespace with: wrangler kv:namespace create JOB_ANALYSIS_CACHE
# For production: wrangler kv:namespace create JOB_ANALYSIS_CACHE --env production

# Job Compatibility Analysis Cache
# - Stores expensive AI-generated compatibility analyses
# - 7-day TTL to balance freshness vs. cost savings
# - Key format: job-analysis:{userId}:{jobId}
[[kv_namespaces]]
binding = "JOB_ANALYSIS_CACHE"
id = "placeholder-id-run-wrangler-kv-namespace-create"
preview_id = "placeholder-preview-id"

# Production environment uses separate namespace
[env.production.kv_namespaces]
binding = "JOB_ANALYSIS_CACHE"
id = "placeholder-production-id"

# D1 Database (if migrating to D1 in future)
# [[d1_databases]]
# binding = "DB"
# database_name = "jobmatch"
# database_id = "your-d1-database-id"

# R2 Storage (if migrating from Supabase Storage in future)
# [[r2_buckets]]
# binding = "STORAGE"
# bucket_name = "jobmatch-files"

# Durable Objects (for distributed rate limiting - future)
# [[durable_objects.bindings]]
# name = "RATE_LIMITER"
# class_name = "RateLimiter"

# Secrets (configure via wrangler secret put)
# Required secrets:
# - SUPABASE_URL
# - SUPABASE_ANON_KEY
# - SUPABASE_SERVICE_ROLE_KEY
# - OPENAI_API_KEY
# - SENDGRID_API_KEY
# - LINKEDIN_CLIENT_ID
# - LINKEDIN_CLIENT_SECRET
# - LINKEDIN_REDIRECT_URI
# - APIFY_API_TOKEN
# - APP_URL
#
# AI Gateway secrets (optional, enables caching and cost reduction):
# - CLOUDFLARE_ACCOUNT_ID (your Cloudflare account ID)
# - AI_GATEWAY_SLUG (AI Gateway name, e.g., "jobmatch-ai-gateway")
#
# When AI Gateway is configured, all OpenAI requests are automatically
# routed through Cloudflare AI Gateway for:
# - Automatic response caching (60-80% cost reduction)
# - Request analytics and monitoring
# - Rate limiting and cost controls
# - No code changes required - transparent proxy
#
# See docs/AI_GATEWAY_ROLLOUT_PLAN.md for deployment instructions

# Worker Limits
# - 10ms CPU time (paid plans: 50ms)
# - 128MB memory
# - 1MB request/response body
# - 30 second request duration (paid: 300s)

# Performance Tips:
# 1. Use c.executionCtx.waitUntil() for background tasks
# 2. Keep cold start fast by lazy-loading heavy modules
# 3. Use caches.default for response caching
